# -*- coding: utf-8 -*-
"""Mini-Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DtX5qb4OF0pvPmhaLvyxryRVt_0I19Mp

**MINI-PROJECT**
"""

!pip install bokeh #data visualization library for creating interactive plots, dashboards and applications.
!pip install scipy #used for scientific computing
!pip install pyngrok # allows you to expose a local web server to the internet using ngrok, a secure tunneling service.

# IMPORTING THE LIBRARIES
# numpy and pandas for data manipulation
# matplotlib for data visualization
# sklearn.cluster KMeans for clustering analysis

import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.cluster import KMeans

# READING THE DATASET

import io
from google.colab import files
uploaded = files.upload()
df = pd.read_csv(io.BytesIO(uploaded["Mall_Customers.csv"]))

print("\nGIVEN DATASET : ")
print("\n--------------------------------------------------------------\n")
print(df)
print("\n--------------------------------------------------------------")
print()

# Displays first 10 rows of the dataframe
df.head(10)

# Displays last a number of rows of the dataframe
a=int(input("Enter the number of rows: "))
df.tail(a)

# The method returns a summary of statistics for each column in the DataFrame
df.describe()

# Prints information about the DataFrame
df.info()

# Checks whether any value in the DataFrame is null (NaN or None) and returns a Boolean Series indicating which columns have missing values.
df.isnull().any()

# Counts distinct gender
df['Gender'].value_counts()

# Commented out IPython magic to ensure Python compatibility.
# %time used to measure the execution time of reading a CSV file Mall_Customers.csv into a Pandas DataFrame using pd.read_csv() function
print("\nEXECUTION TIME: ")
# %time  df = pd.read_csv('Mall_Customers.csv')

# Displays the dimensions of the DataFrame
print("\nThe shape of dataframe: ")
print(df.shape)

# Creates Numpy array x1 and stores data with all rows and 4th,5th columns of the dataframe.
x1 = df.iloc[:, [3, 4]].values

# Normal Scatter plot
#scatter plots provide an effective way to visually represent and analyze the relationship between two variables in a dataset.
print("\nSCATTER PLOT")
x = df["Annual Income (k$)"] #"Annual Income (k$)" value of 50, it means that their annual income is $50,000.
y = df["Spending Score (1-100)"]

plt.scatter(x , y )
plt.xlabel("Annual Income")
plt.ylabel("Spending Score")
plt.title("PLOT")
plt.show()

import seaborn as sns
#Seaborn is a Python data visualization library based on matplotlib that provides a high-level interface for creating informative and attractive statistical graphics.
sns.set(style = 'whitegrid') # sets the style of the plot to have a white grid background.
sns.distplot(df['Annual Income (k$)']) # distplot() function plots a univariate distribution of observations.
plt.title('Distribution of Annual Income', fontsize = 20)
plt.xlabel('Range of Annual Income')
plt.ylabel('Count')#the frequency or count of observations in each bin of the histogram.
plt.show()

sns.set(style = 'dark') # sets style of plot to dark
sns.distplot(df['Age']) #creates histogram of the age variable using the displot()
plt.title('Distribution of Age', fontsize = 20)
plt.xlabel('Range of Age')
plt.ylabel('Count')
plt.show()

labels = ['Female', 'Male'] #creates alist of 2 labels
size = [260, 240] # creates list of 2 values
colors = ['lightgreen', 'orange'] # sets colours for 2 sections of pie-chart
explode = [0, 0.1] # sets the amount of expolde(distance from center) for the second portion of pie chart

plt.rcParams['figure.figsize'] = (7, 7) #sets size of figure to 7 inches by 7 inches
plt.pie(size, colors = colors, explode = explode, labels = labels, shadow = True, autopct = '%.2f%%')
plt.title('A pie chart Representing the Gender')
plt.axis('off') # turns off axis of chart
plt.legend(title='Gender', loc='upper right')
plt.show()

# Used to plot- bar plot which counts the occurrences of each unique value in 'Age' column
df['Age'].value_counts().plot.bar(figsize = (9, 9))
plt.title("BAR PLOT FOR AGE")
plt.xlabel("Uniques values of age")
plt.ylabel("Count")
plt.show()

# Used to plot- bar plot which counts the occurrences of each unique value in 'Annual Income(k$)' column
df['Annual Income (k$)'].value_counts().plot.bar(figsize = (13, 6))
plt.title("BAR PLOT FOR ANNUAL INCOME")
plt.xlabel("Unique value of Annual Income")
plt.ylabel("Count")
plt.show()

# Used to plot- bar plot which counts the occurrences of each unique value in 'Spending Score(1-100)' column
df['Spending Score (1-100)'].value_counts().plot.bar(figsize = (15, 5))
plt.title("BAR PLOT FOR SPENDING SCORE")
plt.xlabel("Unique value of spending score")
plt.ylabel("Count")
plt.show()

#pairwise scatter plot matrix of all the numerical variables in df
#This type of visualization is useful for quickly exploring the relationships between multiple variables in a dataset and identifying any patterns or trends.
sns.pairplot(df)

"""Matshow"""

# To create a matrix plot of the correlation matrix for all the numerical variables in df.
import matplotlib.pyplot as plt
import pandas as pd
df=pd.read_csv('/content/Mall_Customers.csv')
plt.matshow(df.corr())   #The corr() function computes the pairwise correlation between columns of a DataFrame
plt.colorbar()

# scatter_matrix function from the pandas library to create a scatter plot matrix of all the numerical variables in df.
from pandas.plotting import scatter_matrix

# alpha sets opacity of scatter plot points, figsize - size of figure will be created, diagonal - sets type of diagonal here it is 'kde'
scatter_matrix(df, alpha = 0.3, figsize = (14,8), diagonal = 'kde')   #kde is kernel density estimate
plt.show()  #alpha=0.3 means opacity is 30%

# Creating a heatmap
fig, axis = plt.subplots(figsize=(10, 8))
corr = df.corr()

# mask used to hide upper-triangular of heat-map, cmap sets colour map, square sets shape of cells in hetamap as square, ax speciefies axis
sns.heatmap(corr, mask = np.zeros_like(corr, dtype = np.bool), cmap = sns.diverging_palette(220, 10, as_cmap = True),
            square = True, ax = axis)

# x and y variables are assigned to the 'CustomerID' and 'Annual Income (k$)' columns of the DataFrame
x = df['CustomerID']
y = df['Annual Income (k$)']
plt.xlabel("CUSTOMER ID")
plt.ylabel("ANNUAL INCOME")
plt.title("PLOT")
plt.plot(x, y)

# x and y variables are assigned to the 'Annual Income(k$)' and 'Age' columns of the DataFrame
x = df['Annual Income (k$)']
y = df['Age']    #overlapping data points make it difficult to distinguish therfore jumbled plot.
plt.xlabel("ANNUAL INCOME")
plt.ylabel("AGE")
plt.title("PLOT")
plt.plot(x, y)

# Enter value of k(number of clusters)
k=int(input("Enter number of clusters: "))
while k==1:
  print("Invalid value of K")
  k=int(input("Enter new value of clusters: "))
kmeans = KMeans(n_clusters=k)
kmeans.fit(x1)

"""Clustering-clustering is a technique of partitioning a set of data points or objects into groups, called clusters Clustering is widely used in various fields such as data mining, image processing, bioinformatics, market research, and social network analysis.

*K-means clustering*
"""

identified_clusters = kmeans.fit_predict(x1) #fit_predict() fits the model to the data and returns the cluster labels for each data point.

 #centroids represent the mean position of the data points within their respective clusters
centroids = kmeans.cluster_centers_ #centroids of each cluster are retrieved

data_with_clusters = df.copy() #copy of orignal dataframe
data_with_clusters['clusters'] = identified_clusters

print("data with clusters",data_with_clusters)

plt.scatter(data_with_clusters['Annual Income (k$)'] , data_with_clusters['Spending Score (1-100)'],c=data_with_clusters['clusters'],cmap='viridis',label="luster",marker="*")
plt.scatter(centroids[:,0],centroids[:,1],c='blue',s=100,alpha=0.9) #creates a scatter plot of the cluster centroids on the same plot as the data points
plt.title("Plot with clusters")
plt.xlabel("ANNUAL INCOME")
plt.ylabel("SPENDING SCORE")
plt.legend()
plt.show()

"""Elbow Method

In this method, a curve is drawn between “within the sum of squares” (WSS) and the number of clusters.
"""

#Importing reguired libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
import seaborn as sns

# load the dataset
df = pd.read_csv('Mall_Customers.csv')

# extract the relevant features for clustering (i.e. Annual Income and Spending Score)
X = df.iloc[:, [3, 4]].values

# determine the optimal number of clusters using the elbow method
# calculates the Within-Cluster-Sum-of-Squares (WCSS) for different numbers of clusters, and plots the results to create an elbow curve.
wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)
    kmeans.fit(X)
    wcss.append(kmeans.inertia_) #.inertia() attribute used to retireve the WCSS value for that particular number of clusters
    # represents the sum of squared distances of samples to their closest cluster center.

# plot the elbow curve
sns.set(style = 'whitegrid')
plt.plot(range(1, 11), wcss)
plt.title('Elbow Method')
plt.xlabel('Number of Clusters')
plt.ylabel('WCSS')
plt.show()

"""MEAN Shift clustering
(It is an iterative clustering technique that does not require any prior information about the number of clusters in the dataset.)
"""

from sklearn.cluster import MeanShift, estimate_bandwidth
import numpy as np

# estimate the bandwidth (bandwidth is a parameter for MeanShift)
bandwidth = estimate_bandwidth(X, quantile=0.2, n_samples=len(X))
# The quantile parameter is used to set the fraction of the dataset to be included in the kernel density estimation
#n_samples parameter sets the number of samples to be used for the estimation.

# apply MeanShift clustering
ms = MeanShift(bandwidth=bandwidth, bin_seeding=True)# bin_seeding parameter is set to True to speed up the clustering process.
ms.fit(X) # Fitting the MeanShift clustering algorithm to the dataset X.


# extract the labels and cluster centers
labels = ms.labels_  # Extracting the cluster labels for each data point in the dataset.
# Extracting the coordinates of the cluster centers.
cluster_centers = ms.cluster_centers_

# determine the number of clusters
n_clusters_ = len(np.unique(labels))

# print the number of clusters
print("Number of estimated clusters:", n_clusters_)

# plot the resulting clusters
colors = ['r', 'b', 'g', 'y', 'c', 'm']
for i in range(n_clusters_):
    plt.scatter(X[labels == i, 0], X[labels == i, 1], s=50, c=colors[i], label='Cluster %d' % i)

plt.scatter(cluster_centers[:, 0], cluster_centers[:, 1], s=100, c='black', label='Centroids',marker="*")
plt.title('MeanShift Clustering')
plt.xlabel('Annual Income (k$)')
plt.ylabel('Spending Score (1-100)')
plt.legend()
plt.show()

"""Hierarchial clustering"""

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import AgglomerativeClustering
from sklearn.preprocessing import StandardScaler
from scipy.spatial.distance import pdist
from scipy.cluster.hierarchy import dendrogram, linkage
#Agglomerative clustering iteratively merges the two nearest clusters or data points based on some distance metric until all points or clusters are in a single cluster.
# Load the dataset
data = pd.read_csv("Mall_Customers.csv")

# Select features for clustering
X = data.iloc[:,3:5].values     # Considering annual income and spending score

# Standardize the data
scaler = StandardScaler()   #mean=0 and variance=1
X = scaler.fit_transform(X)

# Compute pairwise Euclidean distances
distances = pdist(X, metric='euclidean')    #pdist calculates pairwise distances between the observations.

# Plot dendrogram to determine optimal number of clusters
Z = linkage(distances, method='ward')   #The linkage function is used to determine which clusters should be merged at each step of the algorithm based on a linkage criterion.
dendrogram(Z)
plt.title("Dendrogram")
plt.xlabel("Customers")
plt.ylabel("Euclidean distances")
plt.show()

# Apply hierarchical clustering
hc = AgglomerativeClustering(n_clusters=5, affinity='euclidean', linkage='ward')
#The ward algorithm is a hierarchical clustering algorithm that seeks to minimize the sum of squared differences within all clusters.
y_hc = hc.fit_predict(X)   # this method will fit and perform predictions over training data

# Visualize the clusters
plt.scatter(X[y_hc==0,0], X[y_hc==0,1], s=100, c='red', label='Cluster 1')
plt.scatter(X[y_hc==1,0], X[y_hc==1,1], s=100, c='blue', label='Cluster 2')
plt.scatter(X[y_hc==2,0], X[y_hc==2,1], s=100, c='green', label='Cluster 3')
plt.scatter(X[y_hc==3,0], X[y_hc==3,1], s=100, c='cyan', label='Cluster 4')
plt.scatter(X[y_hc==4,0], X[y_hc==4,1], s=100, c='magenta', label='Cluster 5')
plt.title("Hierarchical Clustering")
plt.xlabel("Annual Income (k$)")
plt.ylabel("Spending Score (1-100)")
plt.legend()
plt.show()

"""SILHOUETTE METHOD (For comparison of 3 clustering methods)



"""

import numpy as np
import pandas as pd
from sklearn.cluster import KMeans, MeanShift, estimate_bandwidth    #used to estimate the bandwidth of a Gaussian kernel for kernel density estimation (KDE) or mean shift clustering.
from sklearn.metrics import silhouette_score

# load the dataset
df = pd.read_csv('Mall_Customers.csv')

# extract the relevant features for clustering (i.e. Annual Income and Spending Score)
X = df.iloc[:, [3, 4]].values

# apply KMeans clustering
kmeans = KMeans(n_clusters=5, init='k-means++', random_state=42)   #init='k-means++' is a commonly used initialization method in K-Means clustering that can help improve the quality and stability of the clustering results.
kmeans.fit(X)    #random_state ensures that the algorithm generates the same sequence of random numbers each time it is run

# apply MeanShift clustering
bandwidth = estimate_bandwidth(X, quantile=0.2, n_samples=len(X))    # setting quantile=0.2 in Pandas' quantile() function will return the value that is greater than or equal to 20% of the values in the dataset, which can be useful for identifying and removing lower outliers.
ms = MeanShift(bandwidth=bandwidth, bin_seeding=True)
ms.fit(X)

# Apply hierarchical clustering
hc = AgglomerativeClustering(n_clusters=5, affinity='euclidean', linkage='ward')  #affinity is a measure of how similar or dissimilar two data points are.
hc.fit(X)    #Ward linkage is a commonly used linkage criterion in hierarchical clustering that seeks to minimize the variance within each cluster.

# evaluate the performance of KMeans and MeanShift using Silhouette score
kmeans_score = silhouette_score(X, kmeans.labels_)
ms_score = silhouette_score(X, ms.labels_)
hc_score=silhouette_score(X,hc.labels_)

#Silhouette score is a metric used to evaluate the quality of clustering in unsupervised machine learning.
# print the Silhouette score for each algorithm
print('KMeans Silhouette score: %.3f' % kmeans_score)
print('MeanShift Silhouette score: %.3f' % ms_score)
print('Hierarchial Silhouette score: %.3f' %hc_score)

"""Creation of dashboard"""

# Import necessary libraries
import pandas as pd
import numpy as np

from bokeh.io import output_notebook, show
from bokeh.plotting import figure
from bokeh.models import ColumnDataSource
from bokeh.layouts import row, column
from bokeh.models.widgets import Select, TextInput

# Set the output to be displayed inline in the notebook
output_notebook()

# Create a ColumnDataSource object for the data
source = ColumnDataSource(df)

# Create the first plot
p1 = figure(title='Age vs. Spending Score', plot_width=500, plot_height=400)
p1.circle('Age', 'Spending Score (1-100)', source=source)

# Create the second plot
p2 = figure(title='Annual Income distribution', plot_width=500, plot_height=400)
hist, edges = np.histogram(df['Annual Income (k$)'], bins=20)
p2.quad(top=hist, bottom=0, left=edges[:-1], right=edges[1:], fill_color='navy', line_color='white')
p2.y_range.start = 0

# Create the dropdown menu widget
menu = Select(options=['Age', 'Annual Income (k$)', 'Spending Score (1-100)'], value='Age', title='X-axis')

# Create the text input widget
text = TextInput(value='10', title='Number of bins')

# Define the callback function for the widgets
def update():
    x_name = menu.value
    num_bins = int(text.value)

    # Update the first plot
    p1.xaxis.axis_label = x_name
    p1.title.text = f'{x_name} vs. Spending Score'

    # Update the second plot
    hist, edges = np.histogram(df[x_name], bins=num_bins)
    p2.quad(top=hist, bottom=0, left=edges[:-1], right=edges[1:], fill_color='navy', line_color='white')

# Add the widgets and plots to the dashboard
menu.on_change('value', lambda attr, old, new: update())
text.on_change('value', lambda attr, old, new: update())

layout = column(menu, text, row(p1, p2))

# Display the dashboard
show(layout)

"""Sample code for creating dashboard of dendogram"""

# Import the required libraries
import pandas as pd
import numpy as np
from scipy.cluster.hierarchy import dendrogram, linkage
from bokeh.io import output_notebook, show
from bokeh.models import ColumnDataSource
from bokeh.plotting import figure
from bokeh.layouts import column, row
from bokeh.models.widgets import Select

# Load the dataset
#df = pd.read_csv('https://raw.githubusercontent.com/kaustubholpadkar/Datasets/main/Mall_Customers.csv')

# Calculate the linkage matrix using Ward's method
Z = linkage(df.iloc[:, 3:], method='ward')

# Set the output to be displayed inline in the notebook
output_notebook()

# Create a ColumnDataSource object for the dendrogram data
source = ColumnDataSource(pd.DataFrame(dendrogram(Z)['icoord'], columns=['x0', 'x1', 'x2', 'x3']))

# Create the plot
p = figure(plot_width=800, plot_height=600, tools='pan, wheel_zoom, reset', title='Dendrogram')

# Add the dendrogram lines to the plot
# Add the dendrogram lines to the plot
dcoord = dendrogram(Z)['dcoord']
ys = []
for i in range(3):
    ys.append(dcoord[:, i])
p.multi_line(xs=[source.data['x0'], source.data['x1'], source.data['x2'], source.data['x3']], ys=ys, line_color='black', line_width=2)

# Create the dropdown menu widget
menu = Select(options=['ward', 'single', 'complete', 'average'], value='ward', title='Linkage Method')

# Define the callback function for the widget
def update(attrname, old, new):
    method = menu.value

    # Update the linkage matrix
    Z = linkage(df.iloc[:, 3:], method=method)

    # Update the dendrogram data source
    source.data = pd.DataFrame(dendrogram(Z)['icoord'], columns=['x0', 'x1', 'x2', 'x3'])

    # Update the plot title
    p.title.text = f'Dendrogram ({method} linkage)'

# Call the update function once to initialize the plot
update(None, None, None)

# Add the widget and plot to the dashboard
menu.on_change('value', update)
layout = column(menu, p)

# Display the dashboard
show(layout)